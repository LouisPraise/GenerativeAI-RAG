{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee26f04b-a078-48eb-88df-de3aeb43f888",
   "metadata": {},
   "source": [
    "**IN THIS PROJECT, I WANNA IMPLEMENT A GenAI and RAG-End-to-End_Machine learning learning project THAT GONNA ASSIST ME IN MY DATA SCIENCE JOURNEY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7770b782-38c4-4d71-831f-e69458a36dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries I'll need for this project\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, StorageContext, load_index_from_storage\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4b7d116-2cce-4bc0-850f-db8d58255aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To do the first step of this project (Chunking), I've decided to use  LlamaIndex instead of langchain. \n",
    "#The reason is that  LlamaIndex is more effective for complex documents such as courses\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=\"./MyCourseraCourses_FromAndrewNg\")\n",
    "documents = reader.load_data()\n",
    "\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=512,  \n",
    "    chunk_overlap=50 \n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6a43ac3-d8f1-4925-b1c6-c9103010efa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents that has been charged : 922\n",
      "Number of segments (chunks) created : 1216\n"
     ]
    }
   ],
   "source": [
    "#Here, I just wanted to make sure that I succeeded on chunking my documents\n",
    "\n",
    "print(f\"Number of documents that has been charged : {len(documents)}\")\n",
    "print(f\"Number of segments (chunks) created : {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb3b34d7-dc45-4cba-9428-68eacc811dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 13:48:32,998 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "#Just after having the confirmation that every thing is okay whith my chunks, \n",
    "#I move to the embedding (second step) using HuggingFace + Groq(my llm-llama3).\n",
    "#The reason is simple, for an education purpose, I prefer to use free and simple local model (OpenAI model require money)\n",
    "\n",
    "os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1' #ignoring window wanning\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "Settings.llm = Groq(model=\"llama-3.3-70b-versatile\")\n",
    "Settings.embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceaae1df-c261-404d-8505-b730f4a84517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-05 13:50:18,750 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your first course appears to be related to machine learning, specifically covering topics such as deciding what to try next when developing a learning algorithm, bias and variance of different learning algorithms, and gradient descent. The course seems to be taught by Andrew Ng and includes practice labs and quizzes to help deepen your understanding of these concepts.\n"
     ]
    }
   ],
   "source": [
    "# Indexing and testing whether or not my RAG work well\n",
    "\n",
    "index = VectorStoreIndex.from_documents(nodes)\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Tell me about my first course ?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95722ccd-2a70-4589-9fe9-79871bdb2eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅Indexes saved in storage folder !\n"
     ]
    }
   ],
   "source": [
    "#Persistence\n",
    "#That means I'm going to create a storage folder and save indexes to disk for instant loading\n",
    "\n",
    "index.storage_context.persist(persist_dir=\"./storage\")\n",
    "print('✅Indexes saved in storage folder !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff1950e-14e3-475d-915f-1ce098143c86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
